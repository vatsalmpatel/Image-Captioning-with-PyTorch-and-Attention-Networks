{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpxNls6iWg/QuGEOUVTBPE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f00bab80bb3741cbb8dd507864f167e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ed18b88df074b3a9067e76f8209d30a",
              "IPY_MODEL_062def4e5caf433c8c201ec6a93a764b",
              "IPY_MODEL_5f1baf1758ac4a07b874dfd59fa98ba5"
            ],
            "layout": "IPY_MODEL_9573a7e0d5474c98ba14a41c61b26a50"
          }
        },
        "7ed18b88df074b3a9067e76f8209d30a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bda3b27cd6844935891dd904b8174638",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4d4762b68b61440f82e1b2d5b9bf5af8",
            "value": "100%"
          }
        },
        "062def4e5caf433c8c201ec6a93a764b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dc957ec07f3484babb880c2dc3ae0ea",
            "max": 178793939,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce8be138bff9423f891d2d8ebd3222fd",
            "value": 178793939
          }
        },
        "5f1baf1758ac4a07b874dfd59fa98ba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5179d4f1b53d4b63b52673fa6eb943d1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d24d8e44b5034d9eb91b2e4104a3a6e4",
            "value": " 171M/171M [00:02&lt;00:00, 71.6MB/s]"
          }
        },
        "9573a7e0d5474c98ba14a41c61b26a50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bda3b27cd6844935891dd904b8174638": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d4762b68b61440f82e1b2d5b9bf5af8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dc957ec07f3484babb880c2dc3ae0ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce8be138bff9423f891d2d8ebd3222fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5179d4f1b53d4b63b52673fa6eb943d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d24d8e44b5034d9eb91b2e4104a3a6e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading dataset and predefined train, test and validation splits"
      ],
      "metadata": {
        "id": "Z-yqluKhfVRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list"
      ],
      "metadata": {
        "id": "2OELG5SPdLMC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "outputId": "83a875d9-1af2-47d7-ab14-f3416181c8db"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1a49d497-425b-4a7f-bb7d-01c707ab4d9f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1a49d497-425b-4a7f-bb7d-01c707ab4d9f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "ref                                                             title                                            size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------------  ----------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "akshaydattatraykhare/diabetes-dataset                           Diabetes Dataset                                  9KB  2022-10-06 08:55:25          11726        367  1.0              \n",
            "whenamancodes/covid-19-coronavirus-pandemic-dataset             COVID -19 Coronavirus Pandemic Dataset           11KB  2022-09-30 04:05:11           9347        288  1.0              \n",
            "stetsondone/video-game-sales-by-genre                           Video Game Sales by Genre                        12KB  2022-10-31 17:56:01            937         23  1.0              \n",
            "whenamancodes/credit-card-customers-prediction                  Credit Card Customers Prediction                379KB  2022-10-30 13:03:27           1571         39  1.0              \n",
            "whenamancodes/students-performance-in-exams                     Students Performance in Exams                     9KB  2022-09-14 15:14:54          16042        298  1.0              \n",
            "akshaydattatraykhare/data-for-admission-in-the-university       Data for Admission in the University              4KB  2022-10-27 11:05:45           2176         50  1.0              \n",
            "michaelbryantds/electric-vehicle-charging-dataset               Electric Vehicle Charging Dataset                98KB  2022-11-02 01:45:23            739         37  0.9411765        \n",
            "maharshipandya/-spotify-tracks-dataset                          ðŸŽ¹ Spotify Tracks Dataset                          8MB  2022-10-22 14:40:15           2373         75  1.0              \n",
            "hasibalmuzdadid/global-air-pollution-dataset                    Global Air Pollution Dataset                    371KB  2022-11-08 14:43:32           1274         48  1.0              \n",
            "whenamancodes/amazon-reviews-on-women-dresses                   Amazon Reviews on Women Dresses                   3MB  2022-10-29 12:47:06            597         32  1.0              \n",
            "akshaydattatraykhare/car-details-dataset                        Car Details Dataset                              56KB  2022-10-21 06:11:56           2844         47  1.0              \n",
            "whenamancodes/international-football-from-1872-to-2022          International Football from 1872 to 2022        572KB  2022-10-30 13:27:29            682         30  0.9411765        \n",
            "whenamancodes/customer-personality-analysis                     Company's Ideal Customers | Marketing Strategy   62KB  2022-10-30 14:17:42           1030         33  1.0              \n",
            "dimitryzub/walmart-coffee-listings-from-500-stores              Walmart Coffee Listings from 500 stores          85KB  2022-10-25 09:20:12           1331         42  1.0              \n",
            "dheerajmukati/most-runs-in-cricket                              Most Runs in International cricket                4KB  2022-10-16 16:49:20            716         32  1.0              \n",
            "thedevastator/food-prices-year-by-year                          Global Food Prices Year By Year                   7KB  2022-10-30 08:49:55            973         31  1.0              \n",
            "saikumartamminana/gold-price-prediction                         Gold Price Prediction                            41KB  2022-10-30 19:07:30           1063         26  0.8235294        \n",
            "thedevastator/udemy-courses-revenue-generation-and-course-anal  Udemy Courses                                   429KB  2022-10-17 00:11:53           2027         57  1.0              \n",
            "whenamancodes/adidas-us-retail-products-dataset                 Adidas US Retail Products Dataset               286KB  2022-10-26 15:44:20            819         33  1.0              \n",
            "jainilcoder/online-payment-fraud-detection                      Online Payment Fraud Detection                  178MB  2022-10-26 12:35:46           1185         40  0.9705882        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d adityajn105/flickr8k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvui9MW7esFD",
        "outputId": "e55d0ede-bba1-4ee7-8956-612b3d41aae3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading flickr8k.zip to /content\n",
            "100% 1.03G/1.04G [00:29<00:00, 38.9MB/s]\n",
            "100% 1.04G/1.04G [00:29<00:00, 37.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip"
      ],
      "metadata": {
        "id": "GyuHjbB9ewE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3edf2a99-0675-43fa-acb1-663adb365a39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-11 18:07:07--  http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip [following]\n",
            "--2022-11-11 18:07:07--  https://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36745453 (35M) [application/zip]\n",
            "Saving to: â€˜caption_datasets.zipâ€™\n",
            "\n",
            "caption_datasets.zi 100%[===================>]  35.04M  12.7MB/s    in 2.8s    \n",
            "\n",
            "2022-11-11 18:07:10 (12.7 MB/s) - â€˜caption_datasets.zipâ€™ saved [36745453/36745453]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q ./caption_datasets.zip"
      ],
      "metadata": {
        "id": "H9h6hgAoe_u0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q flickr8k.zip"
      ],
      "metadata": {
        "id": "gW3ve4NbfFXq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "OJo4YBcVf8Br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "import json\n",
        "import torch\n",
        "from imageio import imread\n",
        "from skimage.transform import resize\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from random import seed, choice, sample"
      ],
      "metadata": {
        "id": "FcLhUZVFfNcy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_input_files(dataset, karpathy_json_path, image_folder, captions_per_image, min_word_freq, output_folder,max_len=100):\n",
        "  assert dataset in {'coco', 'flickr8k', 'flickr30k'}\n",
        "\n",
        "  with open(karpathy_json_path, 'r') as j:\n",
        "      data = json.load(j)\n",
        "  \n",
        "  train_image_paths = []\n",
        "  train_image_captions = []\n",
        "  val_image_paths = []\n",
        "  val_image_captions = []\n",
        "  test_image_paths = []\n",
        "  test_image_captions = []\n",
        "  word_freq = Counter()\n",
        "\n",
        "  for img in data['images']:\n",
        "    captions = []\n",
        "    for c in img['sentences']:\n",
        "        word_freq.update(c['tokens'])\n",
        "        if len(c['tokens']) <= max_len:\n",
        "            captions.append(c['tokens'])\n",
        "\n",
        "    if len(captions) == 0:\n",
        "        continue\n",
        "    \n",
        "    path = os.path.join(image_folder, img['filepath'], img['filename']) if dataset == 'coco' else os.path.join(\n",
        "            image_folder, img['filename'])\n",
        "\n",
        "    if img['split'] in {'train', 'restval'}:\n",
        "        train_image_paths.append(path)\n",
        "        train_image_captions.append(captions)\n",
        "    elif img['split'] in {'val'}:\n",
        "        val_image_paths.append(path)\n",
        "        val_image_captions.append(captions)\n",
        "    elif img['split'] in {'test'}:\n",
        "        test_image_paths.append(path)\n",
        "        test_image_captions.append(captions)\n",
        "\n",
        "  assert len(train_image_paths) == len(train_image_captions)\n",
        "  assert len(val_image_paths) == len(val_image_captions)\n",
        "  assert len(test_image_paths) == len(test_image_captions)\n",
        "\n",
        "  words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
        "  word_map = {k: v + 1 for v, k in enumerate(words)}\n",
        "  word_map['<unk>'] = len(word_map) + 1\n",
        "  word_map['<start>'] = len(word_map) + 1\n",
        "  word_map['<end>'] = len(word_map) + 1\n",
        "  word_map['<pad>'] = 0\n",
        "  \n",
        "  base_filename = dataset + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'\n",
        "\n",
        "  with open(os.path.join(output_folder, 'WORDMAP_' + base_filename + '.json'), 'w') as j:\n",
        "      json.dump(word_map, j)\n",
        "\n",
        "  seed(123)\n",
        "  for impaths, imcaps, split in [(train_image_paths, train_image_captions, 'TRAIN'),\n",
        "                                  (val_image_paths, val_image_captions, 'VAL'),\n",
        "                                  (test_image_paths, test_image_captions, 'TEST')]:\n",
        "\n",
        "      with h5py.File(os.path.join(output_folder, split + '_IMAGES_' + base_filename + '.hdf5'), 'a') as h:\n",
        "          \n",
        "          h.attrs['captions_per_image'] = captions_per_image\n",
        "\n",
        "          \n",
        "          images = h.create_dataset('images', (len(impaths), 3, 256, 256), dtype='uint8')\n",
        "\n",
        "          print(\"\\nReading %s images and captions, storing to file...\\n\" % split)\n",
        "\n",
        "          enc_captions = []\n",
        "          caplens = []\n",
        "\n",
        "          for i, path in enumerate(tqdm(impaths)):\n",
        "\n",
        "              \n",
        "              if len(imcaps[i]) < captions_per_image:\n",
        "                  captions = imcaps[i] + [choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]\n",
        "              else:\n",
        "                  captions = sample(imcaps[i], k=captions_per_image)\n",
        "\n",
        "              \n",
        "              assert len(captions) == captions_per_image\n",
        "\n",
        "              \n",
        "              img = imread(impaths[i])\n",
        "              if len(img.shape) == 2:\n",
        "                  img = img[:, :, np.newaxis]\n",
        "                  img = np.concatenate([img, img, img], axis=2)\n",
        "              img = resize(img, (256, 256))\n",
        "              img = img.transpose(2, 0, 1)\n",
        "              assert img.shape == (3, 256, 256)\n",
        "              assert np.max(img) <= 255\n",
        "\n",
        "              \n",
        "              images[i] = img\n",
        "\n",
        "              for j, c in enumerate(captions):\n",
        "                  \n",
        "                  enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in c] + [\n",
        "                      word_map['<end>']] + [word_map['<pad>']] * (max_len - len(c))\n",
        "\n",
        "                  \n",
        "                  c_len = len(c) + 2\n",
        "\n",
        "                  enc_captions.append(enc_c)\n",
        "                  caplens.append(c_len)\n",
        "\n",
        "          assert images.shape[0] * captions_per_image == len(enc_captions) == len(caplens)\n",
        "\n",
        "          with open(os.path.join(output_folder, split + '_CAPTIONS_' + base_filename + '.json'), 'w') as j:\n",
        "              json.dump(enc_captions, j)\n",
        "\n",
        "          with open(os.path.join(output_folder, split + '_CAPLENS_' + base_filename + '.json'), 'w') as j:\n",
        "              json.dump(caplens, j)"
      ],
      "metadata": {
        "id": "54Ng6eMbf_a5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_embedding(embeddings):\n",
        "    bias = np.sqrt(3.0 / embeddings.size(1))\n",
        "    torch.nn.init.uniform_(embeddings, -bias, bias)"
      ],
      "metadata": {
        "id": "bWswhJDeiZMJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embeddings(emb_file, word_map):\n",
        "    with open(emb_file, 'r') as f:\n",
        "        emb_dim = len(f.readline().split(' ')) - 1\n",
        "\n",
        "    vocab = set(word_map.keys())\n",
        "    embeddings = torch.FloatTensor(len(vocab), emb_dim)\n",
        "    init_embedding(embeddings)\n",
        "    print(\"\\nLoading embeddings...\")\n",
        "    for line in open(emb_file, 'r'):\n",
        "        line = line.split(' ')\n",
        "\n",
        "        emb_word = line[0]\n",
        "        embedding = list(map(lambda t: float(t), filter(lambda n: n and not n.isspace(), line[1:])))\n",
        "        if emb_word not in vocab:\n",
        "            continue\n",
        "\n",
        "        embeddings[word_map[emb_word]] = torch.FloatTensor(embedding)\n",
        "\n",
        "    return embeddings, emb_dim"
      ],
      "metadata": {
        "id": "03cQtrMCjB_r"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_gradient(optimizer, grad_clip):\n",
        "    for group in optimizer.param_groups:\n",
        "        for param in group['params']:\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.clamp_(-grad_clip, grad_clip)"
      ],
      "metadata": {
        "id": "l5meY0JVjJJq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "                    bleu4, is_best):\n",
        "    state = {'epoch': epoch,\n",
        "             'epochs_since_improvement': epochs_since_improvement,\n",
        "             'bleu-4': bleu4,\n",
        "             'encoder': encoder,\n",
        "             'decoder': decoder,\n",
        "             'encoder_optimizer': encoder_optimizer,\n",
        "             'decoder_optimizer': decoder_optimizer}\n",
        "    filename = 'checkpoint_' + data_name + '.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        torch.save(state, 'BEST_' + filename)"
      ],
      "metadata": {
        "id": "K9wAiUGGjLxW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "TyZ9Yys3jQIz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))"
      ],
      "metadata": {
        "id": "PeJWgTArjZe3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(scores, targets, k):\n",
        "    batch_size = targets.size(0)\n",
        "    _, ind = scores.topk(k, 1, True, True)\n",
        "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
        "    correct_total = correct.view(-1).float().sum()\n",
        "    return correct_total.item() * (100.0 / batch_size)"
      ],
      "metadata": {
        "id": "fAjAYWKAjckm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating input files"
      ],
      "metadata": {
        "id": "jQkjjXpdjzIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "create_input_files(\n",
        "    dataset = 'flickr8k',\n",
        "    karpathy_json_path = './dataset_flickr8k.json',\n",
        "    image_folder = './Images/',\n",
        "    captions_per_image = 5,\n",
        "    min_word_freq = 5,\n",
        "    output_folder = './Images/',\n",
        "    max_len = 50\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swj6xfGWjfZP",
        "outputId": "732ead7e-a383-416d-bfcc-9789d1ceebe9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reading TRAIN images and captions, storing to file...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6000/6000 [02:55<00:00, 34.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reading VAL images and captions, storing to file...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:28<00:00, 34.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reading TEST images and captions, storing to file...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:28<00:00, 34.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Dataset for PyTorch using the Dataset class in PyTorch so that the dataset can be used for training, eval and testing"
      ],
      "metadata": {
        "id": "7D3Zo2w6k-2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "Fo8gw4-Pkd0x"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptionDataset(Dataset):\n",
        "  def __init__(self,data_folder,data_name,split,transform = None):\n",
        "    self.split = split\n",
        "    assert self.split in {\"TRAIN\",\"VAL\",\"TEST\"}\n",
        "\n",
        "    self.h = h5py.File(os.path.join(data_folder, self.split + '_IMAGES_' + data_name + '.hdf5'), 'r')\n",
        "    self.imgs = self.h['images']\n",
        "    self.cpi = self.h.attrs['captions_per_image']\n",
        "    with open(os.path.join(data_folder, self.split + '_CAPTIONS_' + data_name + '.json'), 'r') as j:\n",
        "        self.captions = json.load(j)\n",
        "    with open(os.path.join(data_folder, self.split + '_CAPLENS_' + data_name + '.json'), 'r') as j:\n",
        "        self.caplens = json.load(j)\n",
        "    self.transform = transform\n",
        "\n",
        "    self.dataset_size = len(self.captions)\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n",
        "    if self.transform is not None:\n",
        "        img = self.transform(img)\n",
        "\n",
        "    caption = torch.LongTensor(self.captions[i])\n",
        "\n",
        "    caplen = torch.LongTensor([self.caplens[i]])\n",
        "\n",
        "    if self.split is 'TRAIN':\n",
        "        return img, caption, caplen\n",
        "    else:\n",
        "        all_captions = torch.LongTensor(\n",
        "            self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)])\n",
        "        return img, caption, caplen, all_captions\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.dataset_size"
      ],
      "metadata": {
        "id": "rxUlntXTlaEF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Encoder and Decoder Architecture for model training"
      ],
      "metadata": {
        "id": "_8m1z9cymMhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "qbNGH2TGmDcX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\n",
        "    \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "SopgByWnmU2_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv0Lh4jSmd1B",
        "outputId": "3b1fa4ac-deb6-4a25-f5ba-5795315d8b14"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,encoded_image_size = 14):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.enc_image_size = encoded_image_size\n",
        "    resnet = torchvision.models.resnet101(pretrained = True)\n",
        "    modules = list(resnet.children())[:-2]\n",
        "    self.resnet = nn.Sequential(*modules)\n",
        "    self.addaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size,encoded_image_size))\n",
        "    self.fine_tune()\n",
        "  \n",
        "  def forward(self,images):\n",
        "    out = self.resnet(images)\n",
        "    out = self.addaptive_pool(out)\n",
        "    out = out.permute(0,2,3,1)\n",
        "    return out\n",
        "  \n",
        "  def fine_tune(self,fine_tune = True):\n",
        "    for p in self.resnet.parameters():\n",
        "      p.requires_grad = False\n",
        "    for c in list(self.resnet.children())[5:]:\n",
        "      for p in c.parameters():\n",
        "        p.required_grad = fine_tune"
      ],
      "metadata": {
        "id": "UX3-V6ejmfBG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self,encoder_dim,decoder_dim,attention_dim):\n",
        "    super(Attention,self).__init__()\n",
        "    self.encoder_att = nn.Linear(encoder_dim,attention_dim)\n",
        "    self.decoder_att = nn.Linear(decoder_dim,attention_dim)\n",
        "    self.full_att = nn.Linear(attention_dim,1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.Softmax(dim = 1)\n",
        "  \n",
        "  def forward(self,encoder_out,decoder_hidden):\n",
        "    att1 = self.encoder_att(encoder_out)\n",
        "    att2 = self.decoder_att(decoder_hidden)\n",
        "    att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n",
        "    alpha = self.softmax(att)\n",
        "    attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim = 1)\n",
        "    return attention_weighted_encoding, alpha"
      ],
      "metadata": {
        "id": "W_bXem6Imkn_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderWithAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        embeddings = self.embedding(encoded_captions) \n",
        "        h, c = self.init_hidden_state(encoder_out)\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                h[:batch_size_t])\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  \n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))\n",
        "            preds = self.fc(self.dropout(h))\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "metadata": {
        "id": "yLiTPLtSmnaM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "tufJRdw1sxpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "metadata": {
        "id": "eV_38iiqs-8G"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = './Images'\n",
        "data_name = 'flickr8k_5_cap_per_img_5_min_word_freq'\n",
        "\n",
        "emb_dim = 512 \n",
        "attention_dim = 512 \n",
        "decoder_dim = 512\n",
        "dropout = 0.5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cudnn.benchmark = True \n",
        "\n",
        "\n",
        "start_epoch = 0\n",
        "epochs = 10 \n",
        "epochs_since_improvement = 0\n",
        "batch_size = 32\n",
        "workers = 1 \n",
        "encoder_lr = 1e-4\n",
        "decoder_lr = 4e-4\n",
        "grad_clip = 5.\n",
        "alpha_c = 1. \n",
        "best_bleu4 = 0. \n",
        "print_freq = 100\n",
        "fine_tune_encoder = False \n",
        "checkpoint = None"
      ],
      "metadata": {
        "id": "qCfETXv5wZyf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    global best_bleu4, epochs_since_improvement, checkpoint, start_epoch, fine_tune_encoder, data_name, word_map\n",
        "\n",
        "    word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n",
        "    with open(word_map_file, 'r') as j:\n",
        "        word_map = json.load(j)\n",
        "\n",
        "    if checkpoint is None:\n",
        "        decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
        "                                       embed_dim=emb_dim,\n",
        "                                       decoder_dim=decoder_dim,\n",
        "                                       vocab_size=len(word_map),\n",
        "                                       dropout=dropout)\n",
        "        decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
        "                                             lr=decoder_lr)\n",
        "        encoder = Encoder()\n",
        "        encoder.fine_tune(fine_tune_encoder)\n",
        "        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "                                             lr=encoder_lr) if fine_tune_encoder else None\n",
        "\n",
        "    else:\n",
        "        checkpoint = torch.load(checkpoint)\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
        "        best_bleu4 = checkpoint['bleu-4']\n",
        "        decoder = checkpoint['decoder']\n",
        "        decoder_optimizer = checkpoint['decoder_optimizer']\n",
        "        encoder = checkpoint['encoder']\n",
        "        encoder_optimizer = checkpoint['encoder_optimizer']\n",
        "        if fine_tune_encoder is True and encoder_optimizer is None:\n",
        "            encoder.fine_tune(fine_tune_encoder)\n",
        "            encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "                                                 lr=encoder_lr)\n",
        "\n",
        "    decoder = decoder.to(device)\n",
        "    encoder = encoder.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        CaptionDataset(data_folder, data_name, 'TRAIN', transform=transforms.Compose([normalize])),\n",
        "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        CaptionDataset(data_folder, data_name, 'VAL', transform=transforms.Compose([normalize])),\n",
        "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "\n",
        "        \n",
        "        if epochs_since_improvement == 20:\n",
        "            break\n",
        "        if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
        "            adjust_learning_rate(decoder_optimizer, 0.8)\n",
        "            if fine_tune_encoder:\n",
        "                adjust_learning_rate(encoder_optimizer, 0.8)\n",
        "\n",
        "        \n",
        "        train(train_loader=train_loader,\n",
        "              encoder=encoder,\n",
        "              decoder=decoder,\n",
        "              criterion=criterion,\n",
        "              encoder_optimizer=encoder_optimizer,\n",
        "              decoder_optimizer=decoder_optimizer,\n",
        "              epoch=epoch)\n",
        "\n",
        "        \n",
        "        recent_bleu4 = validate(val_loader=val_loader,\n",
        "                                encoder=encoder,\n",
        "                                decoder=decoder,\n",
        "                                criterion=criterion)\n",
        "\n",
        "        \n",
        "        is_best = recent_bleu4 > best_bleu4\n",
        "        best_bleu4 = max(recent_bleu4, best_bleu4)\n",
        "        if not is_best:\n",
        "            epochs_since_improvement += 1\n",
        "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "        else:\n",
        "            epochs_since_improvement = 0\n",
        "\n",
        "        \n",
        "        save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n",
        "                        decoder_optimizer, recent_bleu4, is_best)\n",
        "\n",
        "\n",
        "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n",
        "\n",
        "\n",
        "    decoder.train()\n",
        "    encoder.train()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter() \n",
        "    top5accs = AverageMeter()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "\n",
        "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - start)\n",
        "\n",
        "      \n",
        "        imgs = imgs.to(device)\n",
        "        caps = caps.to(device)\n",
        "        caplens = caplens.to(device)\n",
        "\n",
        "\n",
        "        imgs = encoder(imgs)\n",
        "        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
        "\n",
        "        \n",
        "        targets = caps_sorted[:, 1:]\n",
        "\n",
        "        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0]\n",
        "        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0]\n",
        "\n",
        "        \n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        \n",
        "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "\n",
        "        decoder_optimizer.zero_grad()\n",
        "        if encoder_optimizer is not None:\n",
        "            encoder_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        if grad_clip is not None:\n",
        "            clip_gradient(decoder_optimizer, grad_clip)\n",
        "            if encoder_optimizer is not None:\n",
        "                clip_gradient(encoder_optimizer, grad_clip)\n",
        "\n",
        "\n",
        "        decoder_optimizer.step()\n",
        "        if encoder_optimizer is not None:\n",
        "            encoder_optimizer.step()\n",
        "\n",
        "        top5 = accuracy(scores, targets, 5)\n",
        "        losses.update(loss.item(), sum(decode_lengths))\n",
        "        top5accs.update(top5, sum(decode_lengths))\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n",
        "                                                                          batch_time=batch_time,\n",
        "                                                                          data_time=data_time, loss=losses,\n",
        "                                                                          top5=top5accs))\n",
        "\n",
        "\n",
        "def validate(val_loader, encoder, decoder, criterion):\n",
        "\n",
        "    decoder.eval()\n",
        "    if encoder is not None:\n",
        "        encoder.eval()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top5accs = AverageMeter()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    references = list()  \n",
        "    hypotheses = list()  \n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n",
        "\n",
        "        \n",
        "            imgs = imgs.to(device)\n",
        "            caps = caps.to(device)\n",
        "            caplens = caplens.to(device)\n",
        "\n",
        "        \n",
        "            if encoder is not None:\n",
        "                imgs = encoder(imgs)\n",
        "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
        "\n",
        "        \n",
        "            targets = caps_sorted[:, 1:]\n",
        "\n",
        "        \n",
        "            scores_copy = scores.clone()\n",
        "            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0]\n",
        "            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0]\n",
        "\n",
        "        \n",
        "            loss = criterion(scores, targets)\n",
        "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "            \n",
        "            losses.update(loss.item(), sum(decode_lengths))\n",
        "            top5 = accuracy(scores, targets, 5)\n",
        "            top5accs.update(top5, sum(decode_lengths))\n",
        "            batch_time.update(time.time() - start)\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            if i % print_freq == 0:\n",
        "                print('Validation: [{0}/{1}]\\t'\n",
        "                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n",
        "                                                                                loss=losses, top5=top5accs))\n",
        "            allcaps = allcaps[sort_ind] \n",
        "            for j in range(allcaps.shape[0]):\n",
        "                img_caps = allcaps[j].tolist()\n",
        "                img_captions = list(\n",
        "                    map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n",
        "                        img_caps))\n",
        "                references.append(img_captions)\n",
        "\n",
        "            _, preds = torch.max(scores_copy, dim=2)\n",
        "            preds = preds.tolist()\n",
        "            temp_preds = list()\n",
        "            for j, p in enumerate(preds):\n",
        "                temp_preds.append(preds[j][:decode_lengths[j]])\n",
        "            preds = temp_preds\n",
        "            hypotheses.extend(preds)\n",
        "\n",
        "            assert len(references) == len(hypotheses)\n",
        "\n",
        "        # Calculate BLEU-4 scores\n",
        "        bleu4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "        print(\n",
        "            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n",
        "                loss=losses,\n",
        "                top5=top5accs,\n",
        "                bleu=bleu4))\n",
        "\n",
        "    return bleu4"
      ],
      "metadata": {
        "id": "QN3etzTyyJdW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f00bab80bb3741cbb8dd507864f167e4",
            "7ed18b88df074b3a9067e76f8209d30a",
            "062def4e5caf433c8c201ec6a93a764b",
            "5f1baf1758ac4a07b874dfd59fa98ba5",
            "9573a7e0d5474c98ba14a41c61b26a50",
            "bda3b27cd6844935891dd904b8174638",
            "4d4762b68b61440f82e1b2d5b9bf5af8",
            "9dc957ec07f3484babb880c2dc3ae0ea",
            "ce8be138bff9423f891d2d8ebd3222fd",
            "5179d4f1b53d4b63b52673fa6eb943d1",
            "d24d8e44b5034d9eb91b2e4104a3a6e4"
          ]
        },
        "id": "jvJSlr6ly9kW",
        "outputId": "2e8176c3-0ff5-4bc6-f191-267c0424c697"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/171M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f00bab80bb3741cbb8dd507864f167e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][0/938]\tBatch Time 8.702 (8.702)\tData Load Time 0.256 (0.256)\tLoss 8.7894 (8.7894)\tTop-5 Accuracy 0.504 (0.504)\n",
            "Epoch: [0][100/938]\tBatch Time 0.343 (0.410)\tData Load Time 0.000 (0.003)\tLoss 5.9680 (6.1438)\tTop-5 Accuracy 34.300 (34.681)\n",
            "Epoch: [0][200/938]\tBatch Time 0.332 (0.373)\tData Load Time 0.000 (0.002)\tLoss 5.5006 (5.8033)\tTop-5 Accuracy 43.869 (38.654)\n",
            "Epoch: [0][300/938]\tBatch Time 0.361 (0.365)\tData Load Time 0.000 (0.001)\tLoss 4.8140 (5.5741)\tTop-5 Accuracy 52.356 (41.883)\n",
            "Epoch: [0][400/938]\tBatch Time 0.355 (0.364)\tData Load Time 0.003 (0.001)\tLoss 4.9697 (5.4140)\tTop-5 Accuracy 51.554 (44.015)\n",
            "Epoch: [0][500/938]\tBatch Time 0.365 (0.362)\tData Load Time 0.000 (0.001)\tLoss 4.6906 (5.2941)\tTop-5 Accuracy 53.960 (45.582)\n",
            "Epoch: [0][600/938]\tBatch Time 0.357 (0.361)\tData Load Time 0.000 (0.001)\tLoss 4.9224 (5.1974)\tTop-5 Accuracy 50.139 (46.848)\n",
            "Epoch: [0][700/938]\tBatch Time 0.372 (0.360)\tData Load Time 0.000 (0.001)\tLoss 4.6673 (5.1257)\tTop-5 Accuracy 51.508 (47.769)\n",
            "Epoch: [0][800/938]\tBatch Time 0.365 (0.359)\tData Load Time 0.000 (0.001)\tLoss 4.4296 (5.0624)\tTop-5 Accuracy 57.364 (48.567)\n",
            "Epoch: [0][900/938]\tBatch Time 0.360 (0.359)\tData Load Time 0.000 (0.001)\tLoss 4.7077 (5.0077)\tTop-5 Accuracy 52.368 (49.242)\n",
            "Validation: [0/157]\tBatch Time 0.400 (0.400)\tLoss 4.3647 (4.3647)\tTop-5 Accuracy 59.654 (59.654)\t\n",
            "Validation: [100/157]\tBatch Time 0.261 (0.264)\tLoss 4.3712 (4.4347)\tTop-5 Accuracy 55.585 (56.787)\t\n",
            "\n",
            " * LOSS - 4.440, TOP-5 ACCURACY - 56.703, BLEU-4 - 0.06982907916055461\n",
            "\n",
            "Epoch: [1][0/938]\tBatch Time 0.567 (0.567)\tData Load Time 0.179 (0.179)\tLoss 4.3203 (4.3203)\tTop-5 Accuracy 56.657 (56.657)\n",
            "Epoch: [1][100/938]\tBatch Time 0.363 (0.359)\tData Load Time 0.000 (0.002)\tLoss 4.3820 (4.4648)\tTop-5 Accuracy 57.447 (56.321)\n",
            "Epoch: [1][200/938]\tBatch Time 0.342 (0.357)\tData Load Time 0.000 (0.001)\tLoss 4.6584 (4.4560)\tTop-5 Accuracy 54.085 (56.175)\n",
            "Epoch: [1][300/938]\tBatch Time 0.362 (0.357)\tData Load Time 0.001 (0.001)\tLoss 4.5648 (4.4346)\tTop-5 Accuracy 53.005 (56.484)\n",
            "Epoch: [1][400/938]\tBatch Time 0.339 (0.357)\tData Load Time 0.000 (0.001)\tLoss 4.3531 (4.4153)\tTop-5 Accuracy 60.180 (56.739)\n",
            "Epoch: [1][500/938]\tBatch Time 0.358 (0.357)\tData Load Time 0.000 (0.001)\tLoss 4.1300 (4.4003)\tTop-5 Accuracy 59.786 (56.881)\n",
            "Epoch: [1][600/938]\tBatch Time 0.363 (0.356)\tData Load Time 0.000 (0.001)\tLoss 4.3528 (4.3864)\tTop-5 Accuracy 56.311 (56.999)\n",
            "Epoch: [1][700/938]\tBatch Time 0.340 (0.356)\tData Load Time 0.000 (0.001)\tLoss 4.0882 (4.3730)\tTop-5 Accuracy 61.272 (57.141)\n",
            "Epoch: [1][800/938]\tBatch Time 0.366 (0.356)\tData Load Time 0.000 (0.001)\tLoss 4.3193 (4.3617)\tTop-5 Accuracy 60.814 (57.295)\n",
            "Epoch: [1][900/938]\tBatch Time 0.346 (0.356)\tData Load Time 0.000 (0.001)\tLoss 4.3259 (4.3512)\tTop-5 Accuracy 57.650 (57.410)\n",
            "Validation: [0/157]\tBatch Time 0.411 (0.411)\tLoss 4.3464 (4.3464)\tTop-5 Accuracy 57.729 (57.729)\t\n",
            "Validation: [100/157]\tBatch Time 0.266 (0.264)\tLoss 3.8727 (4.2045)\tTop-5 Accuracy 63.702 (59.336)\t\n",
            "\n",
            " * LOSS - 4.209, TOP-5 ACCURACY - 59.400, BLEU-4 - 0.08175053061434796\n",
            "\n",
            "Epoch: [2][0/938]\tBatch Time 0.565 (0.565)\tData Load Time 0.180 (0.180)\tLoss 4.1437 (4.1437)\tTop-5 Accuracy 60.714 (60.714)\n",
            "Epoch: [2][100/938]\tBatch Time 0.356 (0.359)\tData Load Time 0.000 (0.002)\tLoss 4.1931 (4.1782)\tTop-5 Accuracy 60.260 (59.418)\n",
            "Epoch: [2][200/938]\tBatch Time 0.356 (0.358)\tData Load Time 0.000 (0.001)\tLoss 4.0169 (4.1868)\tTop-5 Accuracy 59.481 (59.413)\n",
            "Epoch: [2][300/938]\tBatch Time 0.372 (0.357)\tData Load Time 0.000 (0.001)\tLoss 4.1812 (4.1734)\tTop-5 Accuracy 60.406 (59.475)\n",
            "Epoch: [2][400/938]\tBatch Time 0.367 (0.357)\tData Load Time 0.000 (0.001)\tLoss 4.0846 (4.1703)\tTop-5 Accuracy 58.947 (59.514)\n",
            "Epoch: [2][500/938]\tBatch Time 0.341 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.9465 (4.1630)\tTop-5 Accuracy 61.145 (59.628)\n",
            "Epoch: [2][600/938]\tBatch Time 0.373 (0.357)\tData Load Time 0.000 (0.001)\tLoss 4.0561 (4.1579)\tTop-5 Accuracy 63.433 (59.759)\n",
            "Epoch: [2][700/938]\tBatch Time 0.332 (0.356)\tData Load Time 0.000 (0.001)\tLoss 4.0094 (4.1514)\tTop-5 Accuracy 64.012 (59.850)\n",
            "Epoch: [2][800/938]\tBatch Time 0.359 (0.356)\tData Load Time 0.000 (0.001)\tLoss 4.0207 (4.1455)\tTop-5 Accuracy 62.895 (59.963)\n",
            "Epoch: [2][900/938]\tBatch Time 0.341 (0.356)\tData Load Time 0.000 (0.001)\tLoss 4.0535 (4.1424)\tTop-5 Accuracy 61.471 (59.999)\n",
            "Validation: [0/157]\tBatch Time 0.404 (0.404)\tLoss 4.4622 (4.4622)\tTop-5 Accuracy 58.263 (58.263)\t\n",
            "Validation: [100/157]\tBatch Time 0.258 (0.265)\tLoss 3.9668 (4.1038)\tTop-5 Accuracy 59.116 (60.887)\t\n",
            "\n",
            " * LOSS - 4.087, TOP-5 ACCURACY - 61.112, BLEU-4 - 0.09178975557764527\n",
            "\n",
            "Epoch: [3][0/938]\tBatch Time 0.602 (0.602)\tData Load Time 0.196 (0.196)\tLoss 4.0254 (4.0254)\tTop-5 Accuracy 59.698 (59.698)\n",
            "Epoch: [3][100/938]\tBatch Time 0.352 (0.360)\tData Load Time 0.000 (0.002)\tLoss 4.0067 (4.0351)\tTop-5 Accuracy 62.963 (61.195)\n",
            "Epoch: [3][200/938]\tBatch Time 0.340 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.7300 (4.0356)\tTop-5 Accuracy 68.986 (61.219)\n",
            "Epoch: [3][300/938]\tBatch Time 0.353 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.6901 (4.0227)\tTop-5 Accuracy 66.575 (61.441)\n",
            "Epoch: [3][400/938]\tBatch Time 0.366 (0.357)\tData Load Time 0.000 (0.001)\tLoss 4.0543 (4.0173)\tTop-5 Accuracy 61.500 (61.513)\n",
            "Epoch: [3][500/938]\tBatch Time 0.348 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.8073 (4.0168)\tTop-5 Accuracy 62.983 (61.526)\n",
            "Epoch: [3][600/938]\tBatch Time 0.342 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.9274 (4.0151)\tTop-5 Accuracy 63.456 (61.544)\n",
            "Epoch: [3][700/938]\tBatch Time 0.354 (0.356)\tData Load Time 0.006 (0.001)\tLoss 4.1036 (4.0140)\tTop-5 Accuracy 60.541 (61.566)\n",
            "Epoch: [3][800/938]\tBatch Time 0.353 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.8931 (4.0123)\tTop-5 Accuracy 62.571 (61.638)\n",
            "Epoch: [3][900/938]\tBatch Time 0.352 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.6875 (4.0100)\tTop-5 Accuracy 67.624 (61.661)\n",
            "Validation: [0/157]\tBatch Time 0.405 (0.405)\tLoss 4.0002 (4.0002)\tTop-5 Accuracy 59.517 (59.517)\t\n",
            "Validation: [100/157]\tBatch Time 0.265 (0.264)\tLoss 4.0085 (4.0323)\tTop-5 Accuracy 63.117 (61.950)\t\n",
            "\n",
            " * LOSS - 4.016, TOP-5 ACCURACY - 62.108, BLEU-4 - 0.09386219483106513\n",
            "\n",
            "Epoch: [4][0/938]\tBatch Time 0.581 (0.581)\tData Load Time 0.188 (0.188)\tLoss 3.9791 (3.9791)\tTop-5 Accuracy 58.501 (58.501)\n",
            "Epoch: [4][100/938]\tBatch Time 0.350 (0.358)\tData Load Time 0.000 (0.002)\tLoss 4.0412 (3.9167)\tTop-5 Accuracy 62.117 (63.022)\n",
            "Epoch: [4][200/938]\tBatch Time 0.354 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.8201 (3.9216)\tTop-5 Accuracy 65.909 (62.871)\n",
            "Epoch: [4][300/938]\tBatch Time 0.348 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.9989 (3.9200)\tTop-5 Accuracy 62.943 (62.860)\n",
            "Epoch: [4][400/938]\tBatch Time 0.362 (0.357)\tData Load Time 0.007 (0.001)\tLoss 4.0456 (3.9204)\tTop-5 Accuracy 60.363 (62.886)\n",
            "Epoch: [4][500/938]\tBatch Time 0.357 (0.357)\tData Load Time 0.000 (0.001)\tLoss 4.0677 (3.9192)\tTop-5 Accuracy 62.466 (62.864)\n",
            "Epoch: [4][600/938]\tBatch Time 0.352 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.7852 (3.9168)\tTop-5 Accuracy 65.946 (62.913)\n",
            "Epoch: [4][700/938]\tBatch Time 0.358 (0.356)\tData Load Time 0.000 (0.001)\tLoss 4.0162 (3.9139)\tTop-5 Accuracy 61.323 (62.969)\n",
            "Epoch: [4][800/938]\tBatch Time 0.368 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.9468 (3.9122)\tTop-5 Accuracy 61.758 (63.015)\n",
            "Epoch: [4][900/938]\tBatch Time 0.354 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.8826 (3.9098)\tTop-5 Accuracy 62.500 (63.056)\n",
            "Validation: [0/157]\tBatch Time 0.398 (0.398)\tLoss 3.8801 (3.8801)\tTop-5 Accuracy 63.636 (63.636)\t\n",
            "Validation: [100/157]\tBatch Time 0.262 (0.263)\tLoss 4.1011 (3.9672)\tTop-5 Accuracy 61.518 (62.716)\t\n",
            "\n",
            " * LOSS - 3.967, TOP-5 ACCURACY - 62.661, BLEU-4 - 0.09688617900960583\n",
            "\n",
            "Epoch: [5][0/938]\tBatch Time 0.580 (0.580)\tData Load Time 0.198 (0.198)\tLoss 3.7476 (3.7476)\tTop-5 Accuracy 64.897 (64.897)\n",
            "Epoch: [5][100/938]\tBatch Time 0.373 (0.358)\tData Load Time 0.000 (0.002)\tLoss 3.7557 (3.8054)\tTop-5 Accuracy 65.375 (64.714)\n",
            "Epoch: [5][200/938]\tBatch Time 0.367 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.7876 (3.8286)\tTop-5 Accuracy 64.268 (64.098)\n",
            "Epoch: [5][300/938]\tBatch Time 0.373 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.9981 (3.8314)\tTop-5 Accuracy 60.622 (63.962)\n",
            "Epoch: [5][400/938]\tBatch Time 0.354 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.8986 (3.8328)\tTop-5 Accuracy 65.027 (63.934)\n",
            "Epoch: [5][500/938]\tBatch Time 0.361 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.7944 (3.8334)\tTop-5 Accuracy 62.022 (63.904)\n",
            "Epoch: [5][600/938]\tBatch Time 0.347 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.8105 (3.8315)\tTop-5 Accuracy 64.888 (63.960)\n",
            "Epoch: [5][700/938]\tBatch Time 0.354 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.9052 (3.8332)\tTop-5 Accuracy 64.011 (63.973)\n",
            "Epoch: [5][800/938]\tBatch Time 0.374 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.9003 (3.8326)\tTop-5 Accuracy 62.774 (64.003)\n",
            "Epoch: [5][900/938]\tBatch Time 0.348 (0.356)\tData Load Time 0.000 (0.000)\tLoss 3.7646 (3.8323)\tTop-5 Accuracy 67.021 (64.025)\n",
            "Validation: [0/157]\tBatch Time 0.402 (0.402)\tLoss 3.7545 (3.7545)\tTop-5 Accuracy 67.172 (67.172)\t\n",
            "Validation: [100/157]\tBatch Time 0.264 (0.263)\tLoss 4.1963 (3.9398)\tTop-5 Accuracy 58.031 (63.446)\t\n",
            "\n",
            " * LOSS - 3.939, TOP-5 ACCURACY - 63.288, BLEU-4 - 0.09854014337101889\n",
            "\n",
            "Epoch: [6][0/938]\tBatch Time 0.603 (0.603)\tData Load Time 0.191 (0.191)\tLoss 3.7209 (3.7209)\tTop-5 Accuracy 67.030 (67.030)\n",
            "Epoch: [6][100/938]\tBatch Time 0.345 (0.360)\tData Load Time 0.000 (0.003)\tLoss 3.6992 (3.7419)\tTop-5 Accuracy 66.959 (65.404)\n",
            "Epoch: [6][200/938]\tBatch Time 0.344 (0.358)\tData Load Time 0.000 (0.001)\tLoss 3.7363 (3.7505)\tTop-5 Accuracy 67.227 (65.180)\n",
            "Epoch: [6][300/938]\tBatch Time 0.347 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.8523 (3.7625)\tTop-5 Accuracy 61.892 (65.052)\n",
            "Epoch: [6][400/938]\tBatch Time 0.350 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.8230 (3.7631)\tTop-5 Accuracy 67.033 (65.050)\n",
            "Epoch: [6][500/938]\tBatch Time 0.360 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.6817 (3.7628)\tTop-5 Accuracy 67.949 (65.010)\n",
            "Epoch: [6][600/938]\tBatch Time 0.357 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.7409 (3.7641)\tTop-5 Accuracy 64.844 (64.993)\n",
            "Epoch: [6][700/938]\tBatch Time 0.340 (0.356)\tData Load Time 0.000 (0.001)\tLoss 4.0547 (3.7686)\tTop-5 Accuracy 59.885 (64.905)\n",
            "Epoch: [6][800/938]\tBatch Time 0.349 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.6341 (3.7669)\tTop-5 Accuracy 69.780 (64.941)\n",
            "Epoch: [6][900/938]\tBatch Time 0.351 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.7110 (3.7672)\tTop-5 Accuracy 64.543 (64.958)\n",
            "Validation: [0/157]\tBatch Time 0.402 (0.402)\tLoss 4.1895 (4.1895)\tTop-5 Accuracy 60.947 (60.947)\t\n",
            "Validation: [100/157]\tBatch Time 0.262 (0.262)\tLoss 3.7131 (3.9070)\tTop-5 Accuracy 64.323 (63.612)\t\n",
            "\n",
            " * LOSS - 3.915, TOP-5 ACCURACY - 63.516, BLEU-4 - 0.10020858551562611\n",
            "\n",
            "Epoch: [7][0/938]\tBatch Time 0.623 (0.623)\tData Load Time 0.220 (0.220)\tLoss 3.6209 (3.6209)\tTop-5 Accuracy 70.341 (70.341)\n",
            "Epoch: [7][100/938]\tBatch Time 0.366 (0.357)\tData Load Time 0.000 (0.002)\tLoss 3.7277 (3.6980)\tTop-5 Accuracy 64.428 (66.082)\n",
            "Epoch: [7][200/938]\tBatch Time 0.338 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.5670 (3.6978)\tTop-5 Accuracy 68.994 (65.970)\n",
            "Epoch: [7][300/938]\tBatch Time 0.369 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.7019 (3.6957)\tTop-5 Accuracy 65.049 (65.907)\n",
            "Epoch: [7][400/938]\tBatch Time 0.354 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.5947 (3.7019)\tTop-5 Accuracy 67.467 (65.804)\n",
            "Epoch: [7][500/938]\tBatch Time 0.359 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.6998 (3.7001)\tTop-5 Accuracy 69.634 (65.858)\n",
            "Epoch: [7][600/938]\tBatch Time 0.354 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.7875 (3.7031)\tTop-5 Accuracy 62.567 (65.831)\n",
            "Epoch: [7][700/938]\tBatch Time 0.342 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.6032 (3.7041)\tTop-5 Accuracy 65.507 (65.800)\n",
            "Epoch: [7][800/938]\tBatch Time 0.365 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.6262 (3.7081)\tTop-5 Accuracy 65.879 (65.748)\n",
            "Epoch: [7][900/938]\tBatch Time 0.371 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.6410 (3.7080)\tTop-5 Accuracy 66.581 (65.750)\n",
            "Validation: [0/157]\tBatch Time 0.411 (0.411)\tLoss 3.7269 (3.7269)\tTop-5 Accuracy 66.158 (66.158)\t\n",
            "Validation: [100/157]\tBatch Time 0.278 (0.264)\tLoss 3.7656 (3.9034)\tTop-5 Accuracy 64.583 (63.889)\t\n",
            "\n",
            " * LOSS - 3.898, TOP-5 ACCURACY - 63.858, BLEU-4 - 0.1021598898233613\n",
            "\n",
            "Epoch: [8][0/938]\tBatch Time 0.583 (0.583)\tData Load Time 0.181 (0.181)\tLoss 3.5790 (3.5790)\tTop-5 Accuracy 66.499 (66.499)\n",
            "Epoch: [8][100/938]\tBatch Time 0.349 (0.358)\tData Load Time 0.000 (0.002)\tLoss 3.6012 (3.6302)\tTop-5 Accuracy 66.940 (66.748)\n",
            "Epoch: [8][200/938]\tBatch Time 0.370 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.6506 (3.6356)\tTop-5 Accuracy 66.176 (66.777)\n",
            "Epoch: [8][300/938]\tBatch Time 0.347 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.4415 (3.6453)\tTop-5 Accuracy 69.792 (66.611)\n",
            "Epoch: [8][400/938]\tBatch Time 0.334 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.8651 (3.6384)\tTop-5 Accuracy 62.500 (66.695)\n",
            "Epoch: [8][500/938]\tBatch Time 0.350 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.5249 (3.6417)\tTop-5 Accuracy 70.541 (66.681)\n",
            "Epoch: [8][600/938]\tBatch Time 0.364 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.6863 (3.6485)\tTop-5 Accuracy 66.580 (66.597)\n",
            "Epoch: [8][700/938]\tBatch Time 0.352 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.7069 (3.6500)\tTop-5 Accuracy 66.022 (66.541)\n",
            "Epoch: [8][800/938]\tBatch Time 0.373 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.7304 (3.6491)\tTop-5 Accuracy 64.767 (66.548)\n",
            "Epoch: [8][900/938]\tBatch Time 0.362 (0.356)\tData Load Time 0.000 (0.000)\tLoss 3.6684 (3.6520)\tTop-5 Accuracy 63.544 (66.494)\n",
            "Validation: [0/157]\tBatch Time 0.405 (0.405)\tLoss 3.7527 (3.7527)\tTop-5 Accuracy 65.241 (65.241)\t\n",
            "Validation: [100/157]\tBatch Time 0.267 (0.264)\tLoss 3.9164 (3.8922)\tTop-5 Accuracy 64.532 (63.888)\t\n",
            "\n",
            " * LOSS - 3.898, TOP-5 ACCURACY - 63.879, BLEU-4 - 0.10154999567174786\n",
            "\n",
            "\n",
            "Epochs since last improvement: 1\n",
            "\n",
            "Epoch: [9][0/938]\tBatch Time 0.593 (0.593)\tData Load Time 0.160 (0.160)\tLoss 3.5209 (3.5209)\tTop-5 Accuracy 69.409 (69.409)\n",
            "Epoch: [9][100/938]\tBatch Time 0.361 (0.358)\tData Load Time 0.000 (0.002)\tLoss 3.5430 (3.5651)\tTop-5 Accuracy 68.519 (67.824)\n",
            "Epoch: [9][200/938]\tBatch Time 0.341 (0.357)\tData Load Time 0.000 (0.001)\tLoss 3.4760 (3.5784)\tTop-5 Accuracy 68.182 (67.566)\n",
            "Epoch: [9][300/938]\tBatch Time 0.369 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.7057 (3.5848)\tTop-5 Accuracy 66.414 (67.490)\n",
            "Epoch: [9][400/938]\tBatch Time 0.367 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.5335 (3.5861)\tTop-5 Accuracy 69.923 (67.519)\n",
            "Epoch: [9][500/938]\tBatch Time 0.339 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.6904 (3.5895)\tTop-5 Accuracy 67.341 (67.479)\n",
            "Epoch: [9][600/938]\tBatch Time 0.353 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.4324 (3.5916)\tTop-5 Accuracy 68.241 (67.424)\n",
            "Epoch: [9][700/938]\tBatch Time 0.357 (0.355)\tData Load Time 0.000 (0.000)\tLoss 3.7396 (3.5941)\tTop-5 Accuracy 66.755 (67.382)\n",
            "Epoch: [9][800/938]\tBatch Time 0.361 (0.355)\tData Load Time 0.000 (0.000)\tLoss 3.5521 (3.5987)\tTop-5 Accuracy 66.410 (67.291)\n",
            "Epoch: [9][900/938]\tBatch Time 0.362 (0.356)\tData Load Time 0.000 (0.001)\tLoss 3.7677 (3.6004)\tTop-5 Accuracy 64.872 (67.277)\n",
            "Validation: [0/157]\tBatch Time 0.396 (0.396)\tLoss 3.6455 (3.6455)\tTop-5 Accuracy 66.944 (66.944)\t\n",
            "Validation: [100/157]\tBatch Time 0.260 (0.263)\tLoss 3.7202 (3.8959)\tTop-5 Accuracy 68.435 (63.854)\t\n",
            "\n",
            " * LOSS - 3.891, TOP-5 ACCURACY - 64.063, BLEU-4 - 0.10265486532783065\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "FRpDZgG9IHYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "data_folder = './Images'  \n",
        "data_name = 'flickr8k_5_cap_per_img_5_min_word_freq'  \n",
        "checkpoint = './BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar'\n",
        "word_map_file = './Images/WORDMAP_flickr8k_5_cap_per_img_5_min_word_freq.json' \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
        "cudnn.benchmark = True  \n",
        "\n",
        "\n",
        "checkpoint = torch.load(checkpoint)\n",
        "decoder = checkpoint['decoder']\n",
        "decoder = decoder.to(device)\n",
        "decoder.eval()\n",
        "encoder = checkpoint['encoder']\n",
        "encoder = encoder.to(device)\n",
        "encoder.eval()\n",
        "\n",
        "\n",
        "with open(word_map_file, 'r') as j:\n",
        "    word_map = json.load(j)\n",
        "rev_word_map = {v: k for k, v in word_map.items()}\n",
        "vocab_size = len(word_map)\n",
        "\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "def evaluate(beam_size):\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        CaptionDataset(data_folder, data_name, 'TEST', transform=transforms.Compose([normalize])),\n",
        "        batch_size=1, shuffle=True, num_workers=1, pin_memory=True)\n",
        "    references = list()\n",
        "    hypotheses = list()\n",
        "\n",
        "    for i, (image, caps, caplens, allcaps) in enumerate(\n",
        "            tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n",
        "\n",
        "        k = beam_size\n",
        "\n",
        "        image = image.to(device) \n",
        "\n",
        "        \n",
        "        encoder_out = encoder(image) \n",
        "        enc_image_size = encoder_out.size(1)\n",
        "        encoder_dim = encoder_out.size(3)\n",
        "        encoder_out = encoder_out.view(1, -1, encoder_dim)  \n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  \n",
        "\n",
        "        \n",
        "        k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)\n",
        "\n",
        "        \n",
        "        seqs = k_prev_words\n",
        "\n",
        "        \n",
        "        top_k_scores = torch.zeros(k, 1).to(device)\n",
        "\n",
        "        complete_seqs = list()\n",
        "        complete_seqs_scores = list()\n",
        "\n",
        "        \n",
        "        step = 1\n",
        "        h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "        \n",
        "        while True:\n",
        "\n",
        "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  \n",
        "\n",
        "            awe, _ = decoder.attention(encoder_out, h)  \n",
        "\n",
        "            gate = decoder.sigmoid(decoder.f_beta(h))  \n",
        "            awe = gate * awe\n",
        "\n",
        "            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  \n",
        "\n",
        "            scores = decoder.fc(h)  \n",
        "            scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "    \n",
        "            scores = top_k_scores.expand_as(scores) + scores  \n",
        "\n",
        "            \n",
        "            if step == 1:\n",
        "                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)\n",
        "            else:\n",
        "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)\n",
        "\n",
        "            prev_word_inds = top_k_words / vocab_size \n",
        "            next_word_inds = top_k_words % vocab_size \n",
        "\n",
        "          \n",
        "            seqs = torch.cat([seqs[prev_word_inds.long()], next_word_inds.unsqueeze(1)], dim=1)\n",
        "\n",
        "            \n",
        "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                               next_word != word_map['<end>']]\n",
        "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "          \n",
        "            if len(complete_inds) > 0:\n",
        "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "            k -= len(complete_inds) \n",
        "\n",
        "            \n",
        "            if k == 0:\n",
        "                break\n",
        "            seqs = seqs[incomplete_inds]\n",
        "            h = h[prev_word_inds[incomplete_inds].long()]\n",
        "            c = c[prev_word_inds[incomplete_inds].long()]\n",
        "            encoder_out = encoder_out[prev_word_inds[incomplete_inds].long()]\n",
        "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "          \n",
        "            if step > 50:\n",
        "                break\n",
        "            step += 1\n",
        "\n",
        "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "        seq = complete_seqs[i]\n",
        "\n",
        "  \n",
        "        img_caps = allcaps[0].tolist()\n",
        "        img_captions = list(\n",
        "            map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}],\n",
        "                img_caps))  \n",
        "        references.append(img_captions)\n",
        "\n",
        "        hypotheses.append([w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n",
        "\n",
        "        assert len(references) == len(hypotheses)\n",
        "\n",
        "    bleu4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "    return bleu4"
      ],
      "metadata": {
        "id": "rZwr88X30cGw"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beam_size = 1\n",
        "print(\"\\nBLEU-4 score @ beam size of %d is %.4f.\" % (beam_size, evaluate(beam_size)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMB8bY_9JAsR",
        "outputId": "56a5d8d8-1ba0-4ef3-ea99-3ae54ae5b988"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EVALUATING AT BEAM SIZE 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [03:11<00:00, 26.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLEU-4 score @ beam size of 1 is 0.0727.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Captioning the Image\n",
        "\n",
        "python caption.py --img='path/to/image.jpeg' --model='path/to/BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar' --word_map='path/to/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json' --beam_size=5"
      ],
      "metadata": {
        "id": "4Z4tRmeKLrEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import json\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import skimage.transform\n",
        "from imageio import imread\n",
        "from skimage.transform import resize\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "GlXT_NmYJRkc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "SUI6obDaL_JN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J-3MKgRfMF-0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}